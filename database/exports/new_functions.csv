routine_name,routine_type,data_type,routine_definition
add_columnstore_policy,PROCEDURE,,ts_policy_compression_add
add_compression_policy,FUNCTION,integer,ts_policy_compression_add
add_continuous_aggregate_policy,FUNCTION,integer,ts_policy_refresh_cagg_add
add_dimension,FUNCTION,record,ts_dimension_add_general
add_dimension,FUNCTION,record,ts_dimension_add
add_job,FUNCTION,integer,ts_job_add
add_process_hypertable_invalidations_policy,PROCEDURE,,ts_policy_process_hyper_inval_add
add_reorder_policy,FUNCTION,integer,ts_policy_reorder_add
add_retention_policy,FUNCTION,integer,ts_policy_retention_add
alter_job,FUNCTION,record,ts_job_alter
approximate_row_count,FUNCTION,bigint,"
DECLARE
    mat_ht           REGCLASS = NULL;
    local_table_name       NAME = NULL;
    local_schema_name      NAME = NULL;
    is_compressed    BOOL = FALSE;
    uncompressed_row_count BIGINT = 0;
    compressed_row_count BIGINT = 0;
    local_compressed_hypertable_id INTEGER = 0;
    local_compressed_chunk_id INTEGER = 0;
    compressed_hypertable_oid  OID;
    local_compressed_chunk_oid  OID;
    max_compressed_row_count BIGINT = 1000;
    is_compressed_chunk INTEGER;
BEGIN
    -- Check if input relation is continuous aggregate view then
    -- get the corresponding materialized hypertable and schema name
    SELECT format('%I.%I', ht.schema_name, ht.table_name)::regclass
    INTO mat_ht
    FROM pg_class c
    JOIN pg_namespace n ON (n.OID = c.relnamespace)
    JOIN _timescaledb_catalog.continuous_agg a ON (a.user_view_schema = n.nspname AND a.user_view_name = c.relname)
    JOIN _timescaledb_catalog.hypertable ht ON (a.mat_hypertable_id = ht.id)
    WHERE c.OID = relation;

    IF mat_ht IS NOT NULL THEN
        relation = mat_ht;
    END IF;

    SELECT relname, nspname FROM pg_class c
    INNER JOIN pg_namespace n ON (n.OID = c.relnamespace)
    INTO local_table_name, local_schema_name
    WHERE c.OID = relation;

    -- Check for input relation is Hypertable
    IF EXISTS (SELECT 1
               FROM _timescaledb_catalog.hypertable WHERE table_name = local_table_name AND schema_name = local_schema_name) THEN
        SELECT compressed_hypertable_id FROM _timescaledb_catalog.hypertable INTO local_compressed_hypertable_id
        WHERE table_name = local_table_name AND schema_name = local_schema_name;
        IF local_compressed_hypertable_id IS NOT NULL THEN
           uncompressed_row_count = _timescaledb_functions.get_approx_row_count(relation);

           -- use the compression_chunk_size stats to fetch precompressed num rows
           SELECT COALESCE(SUM(numrows_pre_compression), 0) FROM _timescaledb_catalog.chunk srcch,
                _timescaledb_catalog.compression_chunk_size map, _timescaledb_catalog.hypertable srcht
                INTO compressed_row_count
                WHERE map.chunk_id = srcch.id
                AND srcht.id = srcch.hypertable_id AND srcht.table_name = local_table_name
                AND srcht.schema_name = local_schema_name;

           RETURN (uncompressed_row_count + compressed_row_count);
        ELSE
           uncompressed_row_count = _timescaledb_functions.get_approx_row_count(relation);
           RETURN uncompressed_row_count;
        END IF;
    END IF;
    -- Check for input relation is CHUNK
    IF EXISTS (SELECT 1 FROM _timescaledb_catalog.chunk WHERE table_name = local_table_name AND schema_name = local_schema_name) THEN
        with compressed_chunk as (select 1 as is_compressed_chunk from _timescaledb_catalog.chunk c
        inner join _timescaledb_catalog.hypertable h on (c.hypertable_id = h.compressed_hypertable_id)
        where c.table_name = local_table_name and c.schema_name = local_schema_name ),
        chunk_temp as (select compressed_chunk_id from _timescaledb_catalog.chunk c where c.table_name = local_table_name and c.schema_name = local_schema_name)
        select ct.compressed_chunk_id, cc.is_compressed_chunk from chunk_temp ct LEFT OUTER JOIN compressed_chunk cc ON 1 = 1
        INTO local_compressed_chunk_id, is_compressed_chunk;
        -- 'input is chunk #1';
        IF is_compressed_chunk IS NULL AND local_compressed_chunk_id IS NOT NULL THEN
        -- 'Include both uncompressed  and compressed chunk #2';
            -- use the compression_chunk_size stats to fetch precompressed num rows
            SELECT COALESCE(numrows_pre_compression, 0) FROM _timescaledb_catalog.compression_chunk_size
                INTO compressed_row_count
                WHERE compressed_chunk_id = local_compressed_chunk_id;

            uncompressed_row_count = _timescaledb_functions.get_approx_row_count(relation);
            RETURN (uncompressed_row_count + compressed_row_count);
        ELSIF is_compressed_chunk IS NULL AND local_compressed_chunk_id IS NULL THEN
        -- 'input relation is uncompressed chunk #3';
            uncompressed_row_count = _timescaledb_functions.get_approx_row_count(relation);
            RETURN uncompressed_row_count;
        ELSE
        -- 'compressed chunk only #4';
            -- use the compression_chunk_size stats to fetch precompressed num rows
            SELECT COALESCE(SUM(numrows_pre_compression), 0) FROM _timescaledb_catalog.chunk srcch,
                _timescaledb_catalog.compression_chunk_size map INTO compressed_row_count
                WHERE map.compressed_chunk_id = srcch.id
                AND srcch.table_name = local_table_name AND srcch.schema_name = local_schema_name;
            RETURN compressed_row_count;
        END IF;
    END IF;
    -- Check for input relation is Plain RELATION
    uncompressed_row_count = _timescaledb_functions.get_approx_row_count(relation);
    RETURN uncompressed_row_count;
END;
"
attach_chunk,PROCEDURE,,ts_attach_chunk
attach_tablespace,FUNCTION,void,ts_tablespace_attach
by_hash,FUNCTION,USER-DEFINED,ts_hash_dimension
by_range,FUNCTION,USER-DEFINED,ts_range_dimension
cagg_migrate,PROCEDURE,,"
DECLARE
    _cagg_schema TEXT;
    _cagg_name TEXT;
    _cagg_name_new TEXT;
    _cagg_data _timescaledb_catalog.continuous_agg;
BEGIN
    -- procedures with SET clause cannot execute transaction
    -- control so we adjust search_path in procedure body
    SET LOCAL search_path TO pg_catalog, pg_temp;

    SELECT nspname, relname
    INTO _cagg_schema, _cagg_name
    FROM pg_catalog.pg_class
    JOIN pg_catalog.pg_namespace ON pg_namespace.oid OPERATOR(pg_catalog.=) pg_class.relnamespace
    WHERE pg_class.oid OPERATOR(pg_catalog.=) cagg::pg_catalog.oid;

    -- maximum size of an identifier in Postgres is 63 characters, se we need to left space for '_new'
    _cagg_name_new := pg_catalog.format('%s_new', pg_catalog.substr(_cagg_name, 1, 59));

    -- pre-validate the migration and get some variables
    _cagg_data := _timescaledb_functions.cagg_migrate_pre_validation(_cagg_schema, _cagg_name, _cagg_name_new);

    -- create new migration plan
    CALL _timescaledb_functions.cagg_migrate_create_plan(_cagg_data, _cagg_name_new, override, drop_old);
    COMMIT;

    -- SET LOCAL is only active until end of transaction.
    -- While we could use SET at the start of the function we do not
    -- want to bleed out search_path to caller, so we do SET LOCAL
    -- again after COMMIT
    SET LOCAL search_path TO pg_catalog, pg_temp;

    -- execute the migration plan
    CALL _timescaledb_functions.cagg_migrate_execute_plan(_cagg_data);

    -- Remove chunk metadata when marked as dropped
    PERFORM _timescaledb_functions.remove_dropped_chunk_metadata(_cagg_data.raw_hypertable_id);

    -- finish the migration plan
    UPDATE _timescaledb_catalog.continuous_agg_migrate_plan
    SET end_ts = pg_catalog.clock_timestamp()
    WHERE mat_hypertable_id OPERATOR(pg_catalog.=) _cagg_data.mat_hypertable_id;
END;
"
chunk_columnstore_stats,FUNCTION,record,SELECT * FROM public.chunk_compression_stats($1)
chunk_compression_stats,FUNCTION,record,"
DECLARE
    table_name name;
    schema_name name;
BEGIN
    SELECT
      relname, nspname
    INTO
	    table_name, schema_name
    FROM
        pg_class c
        INNER JOIN pg_namespace n ON (n.OID = c.relnamespace)
        INNER JOIN _timescaledb_catalog.hypertable ht ON (ht.schema_name = n.nspname
                AND ht.table_name = c.relname)
    WHERE
        c.OID = hypertable;

    IF table_name IS NULL THEN
	    RETURN;
	END IF;

  RETURN QUERY
  SELECT
      *,
      NULL::name
  FROM
      _timescaledb_functions.compressed_chunk_local_stats(schema_name, table_name);
END;
"
chunks_detailed_size,FUNCTION,record,"
DECLARE
        table_name       NAME;
        schema_name      NAME;
BEGIN
        SELECT relname, nspname
        INTO table_name, schema_name
        FROM pg_class c
        INNER JOIN pg_namespace n ON (n.OID = c.relnamespace)
        INNER JOIN _timescaledb_catalog.hypertable ht ON (ht.schema_name = n.nspname AND ht.table_name = c.relname)
        WHERE c.OID = hypertable;

        IF table_name IS NULL THEN
            SELECT h.schema_name, h.table_name
            INTO schema_name, table_name
            FROM pg_class c
            INNER JOIN pg_namespace n ON (n.OID = c.relnamespace)
            INNER JOIN _timescaledb_catalog.continuous_agg a ON (a.user_view_schema = n.nspname AND a.user_view_name = c.relname)
            INNER JOIN _timescaledb_catalog.hypertable h ON h.id = a.mat_hypertable_id
            WHERE c.OID = hypertable;

            IF table_name IS NULL THEN
                RETURN;
            END IF;
		END IF;

    RETURN QUERY SELECT chl.chunk_schema, chl.chunk_name, chl.table_bytes, chl.index_bytes,
                        chl.toast_bytes, chl.total_bytes, NULL::NAME
            FROM _timescaledb_functions.chunks_local_size(schema_name, table_name) chl;
END;
"
compress_chunk,FUNCTION,regclass,ts_compress_chunk
convert_to_columnstore,PROCEDURE,,ts_compress_chunk
convert_to_rowstore,PROCEDURE,,ts_decompress_chunk
create_hypertable,FUNCTION,record,ts_hypertable_create_general
create_hypertable,FUNCTION,record,ts_hypertable_create
decompress_chunk,FUNCTION,regclass,ts_decompress_chunk
delete_job,FUNCTION,void,ts_job_delete
detach_chunk,PROCEDURE,,ts_detach_chunk
detach_tablespace,FUNCTION,integer,ts_tablespace_detach
detach_tablespaces,FUNCTION,integer,ts_tablespace_detach_all_from_hypertable
disable_chunk_skipping,FUNCTION,record,ts_chunk_column_stats_disable
drop_chunks,FUNCTION,text,ts_chunk_drop_chunks
enable_chunk_skipping,FUNCTION,record,ts_chunk_column_stats_enable
first,,anyelement,aggregate_dummy
get_telemetry_report,FUNCTION,jsonb,ts_telemetry_get_report_jsonb
histogram,,ARRAY,aggregate_dummy
hypertable_approximate_detailed_size,FUNCTION,record,ts_hypertable_approximate_size
hypertable_approximate_size,FUNCTION,bigint,"
   SELECT sum(total_bytes)::bigint
   FROM public.hypertable_approximate_detailed_size(hypertable);
"
hypertable_columnstore_stats,FUNCTION,record,SELECT * FROM public.hypertable_compression_stats($1)
hypertable_compression_stats,FUNCTION,record,"
	SELECT
        count(*)::bigint AS total_chunks,
        (count(*) FILTER (WHERE ch.compression_status = 'Compressed'))::bigint AS number_compressed_chunks,
        sum(ch.before_compression_table_bytes)::bigint AS before_compression_table_bytes,
        sum(ch.before_compression_index_bytes)::bigint AS before_compression_index_bytes,
        sum(ch.before_compression_toast_bytes)::bigint AS before_compression_toast_bytes,
        sum(ch.before_compression_total_bytes)::bigint AS before_compression_total_bytes,
        sum(ch.after_compression_table_bytes)::bigint AS after_compression_table_bytes,
        sum(ch.after_compression_index_bytes)::bigint AS after_compression_index_bytes,
        sum(ch.after_compression_toast_bytes)::bigint AS after_compression_toast_bytes,
        sum(ch.after_compression_total_bytes)::bigint AS after_compression_total_bytes,
        ch.node_name
    FROM
	    public.chunk_compression_stats(hypertable) ch
    GROUP BY
        ch.node_name;
"
hypertable_detailed_size,FUNCTION,record,"
DECLARE
        table_name       NAME = NULL;
        schema_name      NAME = NULL;
BEGIN
        SELECT relname, nspname
        INTO table_name, schema_name
        FROM pg_class c
        INNER JOIN pg_namespace n ON (n.OID = c.relnamespace)
        INNER JOIN _timescaledb_catalog.hypertable ht ON (ht.schema_name = n.nspname AND ht.table_name = c.relname)
        WHERE c.OID = hypertable;

        IF table_name IS NULL THEN
                SELECT h.schema_name, h.table_name
                INTO schema_name, table_name
                FROM pg_class c
                INNER JOIN pg_namespace n ON (n.OID = c.relnamespace)
                INNER JOIN _timescaledb_catalog.continuous_agg a ON (a.user_view_schema = n.nspname AND a.user_view_name = c.relname)
                INNER JOIN _timescaledb_catalog.hypertable h ON h.id = a.mat_hypertable_id
                WHERE c.OID = hypertable;

	        IF table_name IS NULL THEN
                        RETURN;
                END IF;
        END IF;

			RETURN QUERY
			SELECT *, NULL::name
			FROM _timescaledb_functions.hypertable_local_size(schema_name, table_name);
END;
"
hypertable_index_size,FUNCTION,bigint,"
DECLARE
        ht_index_name       NAME;
        ht_schema_name      NAME;
        ht_name      NAME;
        ht_id INTEGER;
        index_bytes BIGINT;
BEGIN
   SELECT c.relname, cl.relname, nsp.nspname
   INTO ht_index_name, ht_name, ht_schema_name
   FROM pg_class c, pg_index cind, pg_class cl,
        pg_namespace nsp, _timescaledb_catalog.hypertable ht
   WHERE c.oid = cind.indexrelid AND cind.indrelid = cl.oid
         AND cl.relnamespace = nsp.oid AND c.oid = index_name
		 AND ht.schema_name = nsp.nspname ANd ht.table_name = cl.relname;

   IF ht_index_name IS NULL THEN
       RETURN NULL;
   END IF;

   -- get the local size or size of access node indexes
   SELECT il.total_bytes
   INTO index_bytes
   FROM _timescaledb_functions.indexes_local_size(ht_schema_name, ht_index_name) il;

   IF index_bytes IS NULL THEN
       index_bytes = 0;
   END IF;

   RETURN index_bytes;
END;
"
hypertable_size,FUNCTION,bigint,"
   SELECT total_bytes::bigint FROM public.hypertable_detailed_size(hypertable);
"
interpolate,FUNCTION,smallint,ts_gapfill_marker
interpolate,FUNCTION,bigint,ts_gapfill_marker
interpolate,FUNCTION,integer,ts_gapfill_marker
interpolate,FUNCTION,double precision,ts_gapfill_marker
interpolate,FUNCTION,real,ts_gapfill_marker
last,,anyelement,aggregate_dummy
locf,FUNCTION,anyelement,ts_gapfill_marker
merge_chunks,PROCEDURE,,ts_merge_chunks
merge_chunks,PROCEDURE,,ts_merge_two_chunks
move_chunk,FUNCTION,void,ts_move_chunk
recompress_chunk,PROCEDURE,,"
BEGIN
  IF current_setting('timescaledb.enable_deprecation_warnings', true)::bool THEN
    RAISE WARNING 'procedure public.recompress_chunk(regclass,boolean) is deprecated and the functionality is now included in public.compress_chunk. this compatibility function will be removed in a future version.';
  END IF;
  PERFORM public.compress_chunk(chunk, if_not_compressed);
END"
refresh_continuous_aggregate,PROCEDURE,,ts_continuous_agg_refresh
remove_columnstore_policy,PROCEDURE,,ts_policy_compression_remove
remove_compression_policy,FUNCTION,boolean,ts_policy_compression_remove
remove_continuous_aggregate_policy,FUNCTION,void,ts_policy_refresh_cagg_remove
remove_process_hypertable_invalidations_policy,PROCEDURE,,ts_policy_process_hyper_inval_remove
remove_reorder_policy,FUNCTION,void,ts_policy_reorder_remove
remove_retention_policy,FUNCTION,void,ts_policy_retention_remove
reorder_chunk,FUNCTION,void,ts_reorder_chunk
run_job,PROCEDURE,,ts_job_run
set_adaptive_chunking,FUNCTION,record,ts_chunk_adaptive_set
set_chunk_time_interval,FUNCTION,void,ts_dimension_set_interval
set_integer_now_func,FUNCTION,void,ts_hypertable_set_integer_now_func
set_number_partitions,FUNCTION,void,ts_dimension_set_num_slices
set_partitioning_interval,FUNCTION,void,ts_dimension_set_interval
show_chunks,FUNCTION,regclass,ts_chunk_show_chunks
show_tablespaces,FUNCTION,name,ts_tablespace_show
split_chunk,PROCEDURE,,ts_split_chunk
time_bucket,FUNCTION,bigint,ts_int64_bucket
time_bucket,FUNCTION,integer,ts_int32_bucket
time_bucket,FUNCTION,smallint,ts_int16_bucket
time_bucket,FUNCTION,timestamp with time zone,ts_timestamptz_timezone_bucket
time_bucket,FUNCTION,date,ts_date_offset_bucket
time_bucket,FUNCTION,timestamp with time zone,ts_timestamptz_offset_bucket
time_bucket,FUNCTION,timestamp without time zone,ts_timestamp_offset_bucket
time_bucket,FUNCTION,date,ts_date_bucket
time_bucket,FUNCTION,timestamp with time zone,ts_timestamptz_bucket
time_bucket,FUNCTION,timestamp without time zone,ts_timestamp_bucket
time_bucket,FUNCTION,date,ts_date_bucket
time_bucket,FUNCTION,timestamp with time zone,ts_timestamptz_bucket
time_bucket,FUNCTION,bigint,ts_int64_bucket
time_bucket,FUNCTION,integer,ts_int32_bucket
time_bucket,FUNCTION,smallint,ts_int16_bucket
time_bucket,FUNCTION,timestamp without time zone,ts_timestamp_bucket
time_bucket_gapfill,FUNCTION,integer,ts_gapfill_int32_bucket
time_bucket_gapfill,FUNCTION,timestamp with time zone,ts_gapfill_timestamptz_bucket
time_bucket_gapfill,FUNCTION,timestamp without time zone,ts_gapfill_timestamp_bucket
time_bucket_gapfill,FUNCTION,smallint,ts_gapfill_int16_bucket
time_bucket_gapfill,FUNCTION,bigint,ts_gapfill_int64_bucket
time_bucket_gapfill,FUNCTION,date,ts_gapfill_date_bucket
time_bucket_gapfill,FUNCTION,timestamp with time zone,ts_gapfill_timestamptz_timezone_bucket
timescaledb_post_restore,FUNCTION,boolean,"
DECLARE
    db text;
    catalog_version text;
BEGIN
    SELECT m.value INTO catalog_version FROM pg_extension x
    JOIN _timescaledb_catalog.metadata m ON m.key='timescaledb_version'
    WHERE x.extname='timescaledb' AND x.extversion <> m.value;

    -- check that a loaded dump is compatible with the currently running code
    IF FOUND THEN
        RAISE EXCEPTION 'catalog version mismatch, expected ""%"" seen ""%""', '2.21.0', catalog_version;
    END IF;

    SELECT current_database() INTO db;
    EXECUTE format($$ALTER DATABASE %I RESET timescaledb.restoring $$, db);
    -- we cannot use reset here because the reset_val might not be off
    SET timescaledb.restoring TO off;
    PERFORM _timescaledb_functions.restart_background_workers();

    RETURN true;
END
"
timescaledb_pre_restore,FUNCTION,boolean,"
DECLARE
    db text;
BEGIN
    SELECT current_database() INTO db;
    EXECUTE format($$ALTER DATABASE %I SET timescaledb.restoring ='on'$$, db);
    SET SESSION timescaledb.restoring = 'on';
    PERFORM _timescaledb_functions.stop_background_workers();
    RETURN true;
END
"
update_updated_at_column,FUNCTION,trigger,"
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
"
uuid_generate_v1,FUNCTION,uuid,uuid_generate_v1
uuid_generate_v1mc,FUNCTION,uuid,uuid_generate_v1mc
uuid_generate_v3,FUNCTION,uuid,uuid_generate_v3
uuid_generate_v4,FUNCTION,uuid,uuid_generate_v4
uuid_generate_v5,FUNCTION,uuid,uuid_generate_v5
uuid_nil,FUNCTION,uuid,uuid_nil
uuid_ns_dns,FUNCTION,uuid,uuid_ns_dns
uuid_ns_oid,FUNCTION,uuid,uuid_ns_oid
uuid_ns_url,FUNCTION,uuid,uuid_ns_url
uuid_ns_x500,FUNCTION,uuid,uuid_ns_x500
validate_against_schema,FUNCTION,boolean,"
DECLARE
    schema_def jsonb;
    field_name text;
    field_def jsonb;
BEGIN
    -- Get the schema definition
    SELECT definition INTO schema_def
    FROM schemas
    WHERE id = schema_id
    AND valid_to IS NULL;  -- Only use active schemas

    IF schema_def IS NULL THEN
        RAISE EXCEPTION 'Schema % not found or not active', schema_id;
    END IF;

    -- Check required fields
    FOR field_name, field_def IN SELECT * FROM jsonb_each(schema_def)
    LOOP
        IF (field_def->>'required')::boolean THEN
            IF data->field_name IS NULL THEN
                RAISE EXCEPTION 'Required field % is missing', field_name;
            END IF;
        END IF;

        -- Check type if specified
        IF field_def->>'type' IS NOT NULL THEN
            CASE field_def->>'type'
                WHEN 'string' THEN
                    IF jsonb_typeof(data->field_name) != 'string' THEN
                        RAISE EXCEPTION 'Field % must be a string', field_name;
                    END IF;
                WHEN 'number' THEN
                    IF jsonb_typeof(data->field_name) != 'number' THEN
                        RAISE EXCEPTION 'Field % must be a number', field_name;
                    END IF;
                WHEN 'datetime' THEN
                    -- For datetime, we'll just check if it's a string
                    -- that can be parsed as a timestamp
                    IF jsonb_typeof(data->field_name) != 'string' THEN
                        RAISE EXCEPTION 'Field % must be a datetime string', field_name;
                    END IF;
                    BEGIN
                        PERFORM (data->>field_name)::timestamp;
                    EXCEPTION WHEN OTHERS THEN
                        RAISE EXCEPTION 'Field % must be a valid datetime', field_name;
                    END;
                WHEN 'array' THEN
                    IF jsonb_typeof(data->field_name) != 'array' THEN
                        RAISE EXCEPTION 'Field % must be an array', field_name;
                    END IF;
            END CASE;
        END IF;

        -- Check enum values if specified
        IF field_def->'enum' IS NOT NULL THEN
            IF NOT (data->>field_name) = ANY(ARRAY(SELECT jsonb_array_elements_text(field_def->'enum'))) THEN
                RAISE EXCEPTION 'Field % must be one of: %', 
                    field_name, 
                    array_to_string(ARRAY(SELECT jsonb_array_elements_text(field_def->'enum')), ', ');
            END IF;
        END IF;

        -- Check min/max for numbers
        IF field_def->>'type' = 'number' THEN
            IF field_def->>'min' IS NOT NULL AND (data->>field_name)::numeric < (field_def->>'min')::numeric THEN
                RAISE EXCEPTION 'Field % must be greater than or equal to %', field_name, field_def->>'min';
            END IF;
            IF field_def->>'max' IS NOT NULL AND (data->>field_name)::numeric > (field_def->>'max')::numeric THEN
                RAISE EXCEPTION 'Field % must be less than or equal to %', field_name, field_def->>'max';
            END IF;
        END IF;
    END LOOP;

    RETURN true;
END;
"
